{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Looking back\n",
    "I found the neural recordings data to be quite interesting. The real-time, nearly full spatial mapping of a brain region has fascinating implications for understanding nueral mechanisms and plasticity. \n",
    "I'm curious about what additional elements are needed for learning (e.g. whether reward is necessary). If nothing else is required perhaps the\n",
    "\n",
    "The point to answer from the paper discussion is how the mechanistic learning could be seen in data. Once we've classified the neurons we can look at signal which spikes them. What happens when these signals are endlessly repeated? This could be as simple as going back into the data and finding the movie patterns which trigger a particular class. Does the signal repeat consistently?\n",
    "\n",
    "# 2. Coarse-Graining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple dataset\n",
    "Let's make a dataset which consists of two sets. I had some trouble understanding the point of this assignment, but here is what seems like a trivial data set. The first set has high expression of the first five species and the second set has high expression of the last five species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## create a dataset where the first half is big, and the second half is small\n",
    "data = np.zeros((6,10))\n",
    "sig = 0.4\n",
    "big = 2\n",
    "small= 0.2\n",
    "for set_num in range(2):\n",
    "    for rep_num in range(3):\n",
    "        if set_num: \n",
    "            first = small; second = big\n",
    "        else:\n",
    "            first = big; second = small\n",
    "        first_half  =  first + sig*np.random.randn(5)\n",
    "        second_half = second + sig*np.random.randn(5)\n",
    "        abundance = np.append(first_half,second_half)\n",
    "\n",
    "        data[set_num*3 + rep_num] = abundance\n",
    "\n",
    "## make a plot\n",
    "plt.figure()\n",
    "plt.imshow(data,cmap='bwr')\n",
    "plt.xlabel(\"Species number\")\n",
    "plt.ylabel(\"Replicate number\")\n",
    "plt.title(\"Abundance of species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse-graining option 1/2\n",
    "One easy possibility for a coarse graining is the average of the first 5 species. Perhaps we believe these to be 'important' species, e.g. those which breaks down the original sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_grain_1 = np.mean(data[:,:5],axis=1)\n",
    "plt.plot(coarse_grain_1, 'ok')\n",
    "plt.xlabel(\"Replicate index\")\n",
    "plt.ylabel(\"Average abundance in first 5 species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coarse-graining option 2/2\n",
    "The first strategy is a bit ad-hoc. Instead we can use distance to the first cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = data[0]\n",
    "coarse_grain_2 = np.zeros(6)\n",
    "for i,abundance in enumerate(data):\n",
    "    dist = sum(np.sqrt((base-abundance)**2))\n",
    "    coarse_grain_2[i] = dist\n",
    "plt.plot(coarse_grain_2, 'ok')\n",
    "plt.xlabel(\"Replicate Index\")\n",
    "plt.ylabel(\"Distance from replicate 0\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad coarse graining\n",
    "Of course, it's easy to come up with a bad coarse graining via simply averaging over the \"wrong set\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_grain_3 = data.mean(axis=1)\n",
    "plt.plot(coarse_grain_3, 'or')\n",
    "plt.xlabel(\"Replicate index\")\n",
    "plt.ylabel(\"Bad classifier\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad coarse graining 2/2\n",
    "As in the previous case, we can turn a former good metric into a bad one by using the wrong reference state. In the next bad metric we compare each replicate to it's distance to the total average. As we have done in other analysis, perhaps we believe that we should compare to the mean population rather than to a null observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_trial = np.mean(data,axis=0)\n",
    "\n",
    "coarse_grain_4 = np.zeros(6)\n",
    "for i,replicate in enumerate(data):\n",
    "    dist = np.sum(np.sqrt((replicate-average_trial)**2))\n",
    "    coarse_grain_4[i] = dist\n",
    "\n",
    "plt.plot(coarse_grain_4,'or')\n",
    "plt.xlabel(\"Replicate index\")\n",
    "plt.ylabel(\"Distance from global mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing coarse-graining with ANOVA\n",
    "Given some method of coarse-graining into two bins, how can we clearly say whether they are different _enough_. Our default position should be that we don't know whether they are or not, and we consider how strongly our proposed metric distinguishes groups. We should quantify how \"far away\" the two groups are in units of the intrinsic spread of the dataset. This is exactly the idea of analysis of variance. It yields a statstic (an F-statistic) which quantifies how confidently we may reject the null hypothesis that there is no difference between our coarse-grained categories.\n",
    "\n",
    "The strategy of ANOVA compares two variances. Variance, as always, is taken with respect to two means. ANOVA splits the variance into two comparison means. First is the variance of each proposed \"treatments\"- here the results of a coarse-graining- compared to the global, untreated mean. The second variance is every individual point to the global mean. The first variance can be thought of as signal, which is how far my categories are from 'null'. And the second is the background noise of uncategorized data. If the signal to noise ratio is high then the reduction is a legitimate one. More specifically, it is an instance of hypothesis testing. We are quantifying how strongly we reject the null hypothesis that there is no difference between sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneWay_anova(data,verbose=False):\n",
    "    ## calculate F-test by comparing Sum of Squares between group means\n",
    "    ##      and between data points\n",
    "    ## data: should be 2-D \n",
    "    num_groups = data.shape[0]\n",
    "    num_repeats = data.shape[1]\n",
    "   \n",
    "    M = np.mean(data) \n",
    "  \n",
    "    ## SS treatment: compare each group mean to global mean \n",
    "    diff = np.mean(data,axis=1) - M\n",
    "    ss_treat = num_repeats*np.sum( diff**2 )\n",
    "    ms_treat = ss_treat/(num_groups-1)\n",
    "\n",
    "    ## SS err: compare all points to global mean\n",
    "    all_dist = np.apply_along_axis(lambda x: x-np.mean(x),axis=1, arr=data)\n",
    "    ss_err = np.sum( all_dist**2)\n",
    "    ms_err = ss_err/( data.size-num_groups)\n",
    "\n",
    "    #ss_tot = np.sum( (data-M)**2 )\n",
    "    if verbose:\n",
    "        print(ms_treat, ms_err)\n",
    "        print(data)\n",
    "        print(np.sum(data,axis=1))\n",
    "        print(np.var(data,axis=1))\n",
    "    return ms_treat/ms_err\n",
    "##END oneWay_anova   \n",
    "\n",
    "\n",
    "for i,array in enumerate([coarse_grain_1,coarse_grain_2,coarse_grain_3, coarse_grain_4]):\n",
    "    array.shape = 3,2\n",
    "    f_stat = oneWay_anova(array)\n",
    "    c = 'bbrr'[i]\n",
    "    plt.plot(i,f_stat, 'o', color=c)\n",
    "plt.xlabel(\"Coare graining index\")\n",
    "plt.ylabel(\"F-statistic\")\n",
    "plt.xticks([0,1,2,3]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particular strength is that this statistic extends to any number of categores. One simply sums the variance across all the different categories and compares to the same global variation.\n",
    "\n",
    "Even nonlinear relationships within data such as correlations in a thin shell seem to be captured by this metric. However, the \"resolution\" is lower because the treatments are not so distinct.   \n",
    "\n",
    "Adding rare species to the dataset would increase the intrinsic variance and the assigned gropu variance. As such, rare species would tend to wash out the otherwise clear separation. Averaging over the rare finds could eliminate this 'problem', however, it is not clear to me what to make of rarely occuring species. Should these rare species be taken seriously or not (biologically and/or by the metric)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
